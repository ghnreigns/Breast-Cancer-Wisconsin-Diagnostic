{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "471b74a6-768a-4074-9fd1-b9a6808a97b8",
   "metadata": {},
   "source": [
    "# Dependencies and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e4e21348-94cc-444a-9026-436b9550ff55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import defaultdict\n",
    "from typing import Dict, List, Union\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn import (base, decomposition, linear_model, manifold, metrics,\n",
    "                     preprocessing)\n",
    "\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "016e67c9-594a-44c0-99d9-6829672c9a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "class global_config:\n",
    "    \n",
    "    # File Path\n",
    "    raw_data = \"../data/raw/data.csv\"\n",
    "    processed_data_stage_1 = \"../data/processed/data_stage_1.csv\"\n",
    "    processed_data_stage_2 = \"../data/processed/data_stage_2.csv\"\n",
    "    processed_data_stage_3 = \"../data/processed/data_stage_3.csv\"\n",
    "\n",
    "    # Data Information\n",
    "    target = [\"diagnosis\"]\n",
    "    unwanted_cols = [\"id\", \"Unnamed: 32\"]\n",
    "\n",
    "    # Plotting\n",
    "    colors = [\"#fe4a49\", \"#2ab7ca\", \"#fed766\", \"#59981A\"]\n",
    "    cmap_reversed = plt.cm.get_cmap('mako_r')\n",
    "    \n",
    "    # Seed Number\n",
    "    seed = 1992\n",
    "\n",
    "    # Cross Validation\n",
    "    num_folds = 5\n",
    "    cv_schema = \"StratifiedKFold\"\n",
    "    split_size = {\"train_size\": 0.9, \"test_size\": 0.1}\n",
    "\n",
    "\n",
    "def set_seeds(seed: int = 1234) -> None:\n",
    "    \"\"\"Set seeds for reproducibility.\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9ecaa7fe-c355-4380-a603-356d1ba12a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = global_config\n",
    "\n",
    "# set seeding for reproducibility\n",
    "_ = set_seeds(seed = config.seed)\n",
    "\n",
    "# read data\n",
    "df = pd.read_csv(config.processed_data_stage_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9dc733-229b-4909-babc-006eab62c1d6",
   "metadata": {},
   "source": [
    "# Stage 3: Feature Engineering/Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa19f3f3-3940-4e21-b5b8-e4ce0e4e53cc",
   "metadata": {},
   "source": [
    "## Outlier Detection\n",
    "\n",
    "We will just introduce one way to detect and handle outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc33543-0b55-49a0-932c-9e72f39e8a03",
   "metadata": {},
   "source": [
    "We are fully aware that oftentimes practitioner may accidentally cause data leakage during preprocessing, for example, a subtle yet often mistake is to standardize the whole dataset prior to splitting, or performing feature selection prior to modelling using the information of our response/target variable. \n",
    "\n",
    "However, screening predictors for whether they have zero (or near-zero) variance seems pretty harmless, we can take a look and even though we need not take action, we can confirm our hypothesis after modelling. For example, we performed some variance screening and found some"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823a6c4f-c54a-4223-806b-aac3be4ebb50",
   "metadata": {},
   "source": [
    "## Multicollinearity and Feature Selection\n",
    "\n",
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "    <b>Motivation:</b> We need feature selection in certain problems for the following reasons:\n",
    "    <li> Well, one would definitely have heard of the dreaded curse of dimensionality in the journey of learning Machine Learning where having too many predictor/features can lead to overfitting; on the other hand, too many dimensions can cause distance between observations to appear equidistance from one another. observations become harder to cluster â€” believe it or not, too many dimensions causes every observation in your dataset to appear equidistant from all the others, thereby clogging the model's ability to cluster data points (imagine the horror if you use KNN on 1000 dimensions, all the points will be almost the same distance from each other, poor KNN).\n",
    "    <li> In case you have access to Google's GPU clusters, you likely want to train your model faster. Reducing the number predictors can aid this process.\n",
    "    <li> Reducing uninformative features may aid in model's performance, the idea is to remove unnecessary noise from the dataset.\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Multi-Collinearity:</b> Looking back at our dataset, it is clear to me that there are quite a number of features that are correlated with each other, causing multi-collinearity. Multi-Collinearity is an issue in the history of Linear Models, quoting the statement from <a href=\"https://stats.stackexchange.com/questions/1149/is-there-an-intuitive-explanation-why-multicollinearity-is-a-problem-in-linear-r\"> Is there an intuitive explanation why multicollinearity is a problem in linear regression?</a>\n",
    "    \n",
    "> Consider the simplest case where Y is regressed against X and Z and where X and Z are highly positively correlated. Then the effect of X on Y is hard to distinguish from the effect of Z on Y because any increase in X tends to be associated with an increase in Z.\n",
    "\n",
    "We also note that multi-collinearity is not that big of a problem for non-parametric models such as Decision Tree or Random Forests, however, I will attempt to show that it is still best to avoid in this problem setting.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80b4ba6-3922-4caf-96ff-31dc50c0b94f",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\" role=\"alert\">\n",
    "    <b> Alert! Alert! Alert! </b> There are many methods to perform feature selection. Scikit-Learn offers some of the following:\n",
    "    <li> Univariate feature selection.\n",
    "    <li> Recursive feature elimination.\n",
    "    <li> Backward Elimination of features using Hypothesis Testing.  \n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "<div class=\"alert alert-warning\" role=\"alert\">\n",
    "    <b> EMERGENCY! </b> We need to be careful when selecting features before cross-validation. It is therefore, recommended to include feature selection in cross-validation to avoid any \"bias\" introduced before model selection phase! I decided to use the good old Variance Inflation Factor (VIF) as a way to reduce multicollinearity. Unfortunately, there is no out-of-the-box function to integrate into the <code>Pipeline</code> of scikit-learn. Thus, I heavily modified an existing code in order achieve what I want below.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2e26d4-79d6-4354-b21c-b268e21b2a81",
   "metadata": {},
   "source": [
    "A classical way to check for multicollinearity amongst predictors is to calculate the Variable Inflation Factor (VIF). It is simply done by regressing each predictor $\\mathrm{x}_i$ against all other predictors $\\mathrm{x}_j, j \\neq i$. In other words, the VIF for a predictor variable $i$ is given by:\n",
    "\n",
    "$$\\text{VIF}_i = \\dfrac{1}{1 - R^{2}_{i}}$$\n",
    "\n",
    "where $R^{2}_{i}$ is, by definition, the proportion of the variation in the \"dependent variable\" $\\mathrm{x}_i$ that is predictable from the indepedent predictors $\\mathrm{x}_j, j \\neq i$. Consequently, the higher the $R^2_i$ of a predictor, the higher the VIF, and this indicates there is linear dependence among predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb6718d-7983-4fef-bc71-e1b3bb6b67ab",
   "metadata": {},
   "source": [
    "### Version 1: Using Scikit-Learn's Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "55649f04-03ad-4e88-941c-ff453d780bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def VIF(df: pd.DataFrame) -> List[str]:\n",
    "    \"\"\"Takes in a dataframe and outputs a list of VIFs.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): [description]\n",
    "\n",
    "    Returns:\n",
    "        List[str]: [description]\n",
    "    \"\"\"\n",
    "    vif_list = list()\n",
    "    for col in df.columns:\n",
    "        X = df.drop(columns=[col])\n",
    "        y = df[col]\n",
    "        # automatically standard scale/normalize when fit intercept is true\n",
    "        model = linear_model.LinearRegression(fit_intercept=True).fit(X.values, y.values)\n",
    "        ypreds = model.predict(X.values)\n",
    "        r2 = metrics.r2_score(y.values, ypreds)\n",
    "        VIF = 1 / (1 - r2)\n",
    "        vif_list.append(VIF)\n",
    "    return vif_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2fdf1540-4a46-49fd-8903-47f9c4b9d38f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3806.1152963979675\n",
      "616.3508614719424\n",
      "325.64131198187516\n",
      "123.25781086343038\n",
      "64.65479584770004\n",
      "35.61751844352034\n",
      "33.96063880508537\n",
      "30.596655364833975\n",
      "25.387829695531387\n",
      "18.843208489973282\n",
      "17.232376192128665\n",
      "16.333806476471736\n",
      "15.510661467365699\n",
      "8.984359709155799\n"
     ]
    }
   ],
   "source": [
    "predictor_cols = df.columns.to_list()[1:]\n",
    "\n",
    "VIF_max = 100000\n",
    "VIF_dfs = {}\n",
    "n = 0\n",
    "while int(VIF_max) > 10:   \n",
    "    try:\n",
    "        data = data.drop(columns = [temp['Features'][0]])\n",
    "        temp = (pd.DataFrame({'Features':data.columns,'VIF':VIF(data)})\n",
    "        .sort_values(by = 'VIF', ascending = False).reset_index(drop = True))\n",
    "    except:\n",
    "        data = df.drop(columns = ['diagnosis'])\n",
    "        temp = (pd.DataFrame({'Features':df.drop(columns = ['diagnosis']).columns,'VIF':VIF(data)})\n",
    "        .sort_values(by = 'VIF', ascending = False).reset_index(drop = True))\n",
    "    \n",
    "    print(temp['VIF'][0])\n",
    "    VIF_max = temp['VIF'][0]\n",
    "    VIF_dfs['iter_{}'.format(n)] = temp\n",
    "    n+=1\n",
    "del temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6fc91ce6-464e-44bf-b925-101e28be13ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Features</th>\n",
       "      <th>VIF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>concave points_mean</td>\n",
       "      <td>8.984360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>area_worst</td>\n",
       "      <td>8.675765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fractal_dimension_worst</td>\n",
       "      <td>8.485684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>smoothness_worst</td>\n",
       "      <td>8.373798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>symmetry_worst</td>\n",
       "      <td>7.264684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>fractal_dimension_mean</td>\n",
       "      <td>7.168068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>smoothness_mean</td>\n",
       "      <td>6.621582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>compactness_se</td>\n",
       "      <td>5.889510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>fractal_dimension_se</td>\n",
       "      <td>5.734092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>concave points_se</td>\n",
       "      <td>4.440585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>symmetry_se</td>\n",
       "      <td>4.312337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>perimeter_se</td>\n",
       "      <td>4.310524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>concavity_se</td>\n",
       "      <td>3.966669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>symmetry_mean</td>\n",
       "      <td>3.651681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>smoothness_se</td>\n",
       "      <td>3.235873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>texture_se</td>\n",
       "      <td>2.008790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>texture_mean</td>\n",
       "      <td>1.700532</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Features       VIF\n",
       "0       concave points_mean  8.984360\n",
       "1                area_worst  8.675765\n",
       "2   fractal_dimension_worst  8.485684\n",
       "3          smoothness_worst  8.373798\n",
       "4            symmetry_worst  7.264684\n",
       "5    fractal_dimension_mean  7.168068\n",
       "6           smoothness_mean  6.621582\n",
       "7            compactness_se  5.889510\n",
       "8      fractal_dimension_se  5.734092\n",
       "9         concave points_se  4.440585\n",
       "10              symmetry_se  4.312337\n",
       "11             perimeter_se  4.310524\n",
       "12             concavity_se  3.966669\n",
       "13            symmetry_mean  3.651681\n",
       "14            smoothness_se  3.235873\n",
       "15               texture_se  2.008790\n",
       "16             texture_mean  1.700532"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VIF_dfs[f'iter_{n-1}']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f7fea1-5183-4f07-816f-77545f88386e",
   "metadata": {},
   "source": [
    "### Version 2: Using Statsmodels VarianceInflationFactor\n",
    "\n",
    "Note that we need to perform scaling first before fitting our `ReduceVIF` to get the exact same result as the previous version. In this version, I manually added a hard threshold for the number of features remaining to be 15. This hard coded number can be turned into a parameter (hyperparameter) in our pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5e7a9206-e167-4c54-b89e-29042adc540a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReduceVIF(base.BaseEstimator, base.TransformerMixin):\n",
    "    \"\"\"The base of the class structure is not implemented by me, however, I heavily modified the class such that it can\n",
    "    take in numpy arrays and correctly implemented the fit and transform method.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, thresh=10):\n",
    "        self.thresh = thresh\n",
    "        self.feature_names_ = None\n",
    "        self.predictor_cols = [\n",
    "            \"radius_mean\",\n",
    "            \"texture_mean\",\n",
    "            \"perimeter_mean\",\n",
    "            \"area_mean\",\n",
    "            \"smoothness_mean\",\n",
    "            \"compactness_mean\",\n",
    "            \"concavity_mean\",\n",
    "            \"concave points_mean\",\n",
    "            \"symmetry_mean\",\n",
    "            \"fractal_dimension_mean\",\n",
    "            \"radius_se\",\n",
    "            \"texture_se\",\n",
    "            \"perimeter_se\",\n",
    "            \"area_se\",\n",
    "            \"smoothness_se\",\n",
    "            \"compactness_se\",\n",
    "            \"concavity_se\",\n",
    "            \"concave points_se\",\n",
    "            \"symmetry_se\",\n",
    "            \"fractal_dimension_se\",\n",
    "            \"radius_worst\",\n",
    "            \"texture_worst\",\n",
    "            \"perimeter_worst\",\n",
    "            \"area_worst\",\n",
    "            \"smoothness_worst\",\n",
    "            \"compactness_worst\",\n",
    "            \"concavity_worst\",\n",
    "            \"concave points_worst\",\n",
    "            \"symmetry_worst\",\n",
    "            \"fractal_dimension_worst\",\n",
    "        ]\n",
    "\n",
    "    def reset(self):\n",
    "\n",
    "        self.predictor_cols = [\n",
    "            \"radius_mean\",\n",
    "            \"texture_mean\",\n",
    "            \"perimeter_mean\",\n",
    "            \"area_mean\",\n",
    "            \"smoothness_mean\",\n",
    "            \"compactness_mean\",\n",
    "            \"concavity_mean\",\n",
    "            \"concave points_mean\",\n",
    "            \"symmetry_mean\",\n",
    "            \"fractal_dimension_mean\",\n",
    "            \"radius_se\",\n",
    "            \"texture_se\",\n",
    "            \"perimeter_se\",\n",
    "            \"area_se\",\n",
    "            \"smoothness_se\",\n",
    "            \"compactness_se\",\n",
    "            \"concavity_se\",\n",
    "            \"concave points_se\",\n",
    "            \"symmetry_se\",\n",
    "            \"fractal_dimension_se\",\n",
    "            \"radius_worst\",\n",
    "            \"texture_worst\",\n",
    "            \"perimeter_worst\",\n",
    "            \"area_worst\",\n",
    "            \"smoothness_worst\",\n",
    "            \"compactness_worst\",\n",
    "            \"concavity_worst\",\n",
    "            \"concave points_worst\",\n",
    "            \"symmetry_worst\",\n",
    "            \"fractal_dimension_worst\",\n",
    "        ]\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        print(\"ReduceVIF fit\")\n",
    "        tmp, self.predictor_cols = ReduceVIF.calculate_vif(X, self.predictor_cols, self.thresh)\n",
    "        self.feature_names_ = self.predictor_cols  # save as an attribute to call later\n",
    "        col_index = [self.predictor_cols.index(col_name) for col_name in self.predictor_cols]\n",
    "        self.col_index = col_index\n",
    "        self.reset()\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        print(\"ReduceVIF transform\")\n",
    "        # columns = X.columns.tolist()\n",
    "        # print(X.shape)\n",
    "        return X[:, self.col_index]\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_vif(X: Union[np.ndarray, pd.DataFrame], columns: List[str], thresh: float = 10.0):\n",
    "        \"\"\"Implements a VIF function that recursively eliminates features.\n",
    "\n",
    "        Args:\n",
    "            X (Union[np.ndarray, pd.DataFrame]): [description]\n",
    "            columns (List[str]): [description]\n",
    "            thresh (float, optional): [description]. Defaults to 10.0.\n",
    "\n",
    "        Returns:\n",
    "            [type]: [description]\n",
    "        \"\"\"\n",
    "\n",
    "        dropped = True\n",
    "        count = 0\n",
    "        while dropped and count <= 15:\n",
    "            column_index = X.shape[1]\n",
    "            predictor_cols = np.arange(X.shape[1])\n",
    "            dropped = False\n",
    "            vif = []\n",
    "            for var in range(column_index):\n",
    "                # print(predictor_cols.shape)\n",
    "                vif.append(variance_inflation_factor(X[:, predictor_cols], var))\n",
    "\n",
    "            max_vif = max(vif)\n",
    "            if max_vif > thresh:\n",
    "                maxloc = vif.index(max_vif)\n",
    "                print(f\"Dropping {maxloc} with vif={max_vif}\")\n",
    "                # X = X.drop([X.columns.tolist()[maxloc]], axis=1)\n",
    "                X = np.delete(X, maxloc, axis=1)\n",
    "                columns.pop(maxloc)\n",
    "                dropped = True\n",
    "                count += 1\n",
    "        return X, columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c397adc9-c655-486a-b7c0-373d5594286a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ReduceVIF fit\n",
      "Dropping 0 with vif=3806.1152963979675\n",
      "Dropping 19 with vif=616.3508614719424\n",
      "Dropping 1 with vif=325.64131198187516\n",
      "Dropping 19 with vif=123.25781086343038\n",
      "Dropping 4 with vif=64.65479584770004\n",
      "Dropping 7 with vif=35.61751844352034\n",
      "Dropping 19 with vif=33.96063880508537\n",
      "Dropping 20 with vif=30.596655364834078\n",
      "Dropping 1 with vif=25.387829695531387\n",
      "Dropping 2 with vif=18.843208489973282\n",
      "Dropping 14 with vif=17.232376192128665\n",
      "Dropping 7 with vif=16.333806476471736\n",
      "Dropping 15 with vif=15.510661467365699\n",
      "ReduceVIF transform\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['texture_mean',\n",
       " 'smoothness_mean',\n",
       " 'concave points_mean',\n",
       " 'symmetry_mean',\n",
       " 'fractal_dimension_mean',\n",
       " 'texture_se',\n",
       " 'perimeter_se',\n",
       " 'smoothness_se',\n",
       " 'compactness_se',\n",
       " 'concavity_se',\n",
       " 'concave points_se',\n",
       " 'symmetry_se',\n",
       " 'fractal_dimension_se',\n",
       " 'area_worst',\n",
       " 'smoothness_worst',\n",
       " 'symmetry_worst',\n",
       " 'fractal_dimension_worst']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer = ReduceVIF()\n",
    "scaler = preprocessing.StandardScaler()\n",
    "X = scaler.fit_transform(df[predictor_cols])\n",
    "# Only use 10 columns for speed in this example\n",
    "X = transformer.fit_transform(X)\n",
    "transformer.feature_names_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b44d6e3-bd6a-4a55-8780-b10dab085e1f",
   "metadata": {},
   "source": [
    "### Call To Action\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Using VIF in Modelling Pipeline:</b> At this step, we are just showing how we can remove multicollinear features using VIF; but we will not remove them at this point in time. We will incorporate this feature selection technique in our Cross-Validation pipeline in order to avoid data leakage.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a45b17-5801-4a53-baaf-e4d10a346e8d",
   "metadata": {},
   "source": [
    "# Save the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3509e9b9-d143-468a-bd2a-55666481245d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(config.processed_data_stage_3, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
